{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as utils\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = 'data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  8000\n",
      "Test set size:  2000\n",
      "Sample tokenized review:  ['This', 'is', 'an', 'incredible', 'spot', 'in', 'the', 'funk', 'zone', 'of', 'Santa', 'Barbara', '.', 'Their', 'burgers', 'are', 'overpriced', '(', '$', '18/', '$', '14', ')', 'which', 'is', 'why', 'they', 'lost', 'a', 'star', ',', 'otherwise', 'great', 'beer', 'selection', 'and', 'the', 'lobster', 'Mac', 'and', 'cheese', 'was', 'delicious', '.', 'Highly', 'recommend', 'stopping', 'by', '.', 'Beer', 'cooler', 'had', 'a', 'great', 'selection', 'as', 'well', '.']\n",
      "Sample rating:  4\n"
     ]
    }
   ],
   "source": [
    "train_tups, test_tups = utils.split_data(utils.generate_tuples_from_file(DATA_FILE_PATH, num_samples=10000), test_size=0.2)\n",
    "\n",
    "print('Training set size: ', len(train_tups[0]))\n",
    "print('Test set size: ', len(test_tups[0]))\n",
    "print('Sample tokenized review: ', train_tups[0][0])\n",
    "print('Sample rating: ', train_tups[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement FFNN with doc2vec embeddings as input\n",
    "\n",
    "tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_tups[0])]\n",
    "d2v_model = Doc2Vec(tagged_docs, vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "vectors = [d2v_model.dv[i] for i in range(len(tagged_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15405 (60.18 KB)\n",
      "Trainable params: 15405 (60.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import Recall, Precision, F1Score\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(input_dim,)))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# put in an output layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(learning_rate=.01)\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "f1_score = F1Score()\n",
    "\n",
    "# call compile here\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', recall, precision, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input vectors:  (8000, 100)\n",
      "Shape of output vectors:  (8000, 5)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(160,) and logits.shape=(32, 5)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/dpham/Code/NLP-final-project/ffnn.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dpham/Code/NLP-final-project/ffnn.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mShape of output vectors: \u001b[39m\u001b[39m'\u001b[39m, train_tups_np\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dpham/Code/NLP-final-project/ffnn.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dpham/Code/NLP-final-project/ffnn.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(vectors_np, train_tups_np, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/ts/cp2s59lj3s9c803vmldtnh4m0000gn/T/__autograph_generated_filef5hvjr56.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(160,) and logits.shape=(32, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert input data to NumPy arrays\n",
    "vectors_np = np.array(vectors)\n",
    "train_tups_np = np.array(train_tups[1])\n",
    "train_tups_np = utils.get_one_hot_encodings(train_tups_np)\n",
    "\n",
    "print('Shape of input vectors: ', vectors_np.shape)\n",
    "print('Shape of output vectors: ', train_tups_np.shape)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(vectors_np, train_tups_np, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_to_predict = \"The food was awful.\"\n",
    "vectors_np = np.array([d2v_model.infer_vector(review_to_predict.split(' '))])\n",
    "prob_distribution = model.predict(vectors_np)\n",
    "\n",
    "print('Predicted rating: ', np.argmax(prob_distribution) + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
