{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as utils\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = 'data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  8000\n",
      "Test set size:  2000\n",
      "Sample tokenized review:  ['I', 'really', 'ca', \"n't\", 'think', 'of', 'anything', 'bad', 'to', 'say', 'about', 'this', 'place', '!', 'I', 'have', 'eaten', 'there', 'twice', 'and', 'verbally', '(', 'and', 'loudly', ')', 'moaned', 'in', 'food', 'pleasure', 'over', 'each', 'meal', '!', 'Meal', 'One', ':', 'We', 'Appz', \"'d\", 'on', 'the', 'Assorted', 'Breads', '.', 'Safe', 'choice', ',', 'but', 'wonderful', 'selections', '!', 'My', 'dinner', 'entree', 'was', 'the', 'Amish', 'Chicken', '.', 'Very', 'tender', 'and', 'juicy', 'and', 'the', 'potato', 'hash', 'was', 'wonderful', '!', 'Meal', 'Two', ':', 'The', 'Appz', 'was', 'the', 'Potted', 'Pig', ',', 'and', 'the', 'Flash', 'Fried', 'Brussels', 'Sprouts', '.', 'The', 'potted', 'pig', 'was', 'plated', 'creatively', 'having', 'to', 'use', 'a', 'spoon', 'to', 'dive', 'into', 'the', 'pork', '(', 'texture', 'of', 'a', 'country', 'pate', ')', 'and', 'then', 'add', 'your', 'toppings', 'from', 'the', 'pickled', 'selections', 'of', 'veggies', 'they', 'gave', 'you', 'either', 'on', 'a', 'cracker', 'or', 'bread', '.', 'It', 'was', 'good', '....', 'BUT', 'If', 'you', 'REALLY', 'want', 'a', 'surprising', 'treat', ',', 'get', 'the', 'Flash', 'Fried', 'Brussels', 'Sprouts', '!', 'Holy', 'YUM', '.', 'If', 'Brussels', 'Spouts', 'could', 'ever', 'be', 'considered', 'candy', '....', 'you', 'found', 'it', 'here', '.', 'They', 'flash', 'fry', 'them', 'and', 'put', 'them', 'in', 'a', 'dish', 'with', 'onions', ',', 'almonds', 'and', 'bacon', '!', 'What', 'a', 'TREAT', '!', 'I', 'chose', '(', 'after', 'tasting', 'my', 'buddies', 'last', 'visit', ')', 'the', 'Pork', 'Chop', '.', 'And', 'I', 'remembered', 'correctly', ',', 'it', 'was', 'fantastic', '!', 'The', 'pork', 'was', 'perfectly', 'seasoned', 'and', 'cooked', 'and', 'they', 'put', 'it', 'over', 'a', 'bed', 'of', 'sweet', 'potatoes', 'and', 'topped', 'it', 'with', 'apple', 'slaw', '.', 'So', 'many', 'flavors', 'that', 'melded', 'together', 'perfectly', '!', 'and', 'YES', 'I', 'ate', 'my', 'entire', 'plate', '!', 'Dessert', 'we', 'each', 'got', 'one', 'and', 'shared', '-', 'table', 'of', 'four', 'here-just', 'for', 'reference', ';', ')', 'We', 'got', 'the', \"S'mores\", 'pudding', '.', 'They', 'topped', 'with', 'semi', 'burned', 'marshmallows', '.', 'Holy', 'yum', '.', 'and', 'the', 'pudding', 'was', 'topped', 'with', 'candied', 'pecans', '.', 'I', 'can', 'not', 'recall', 'the', 'name', 'of', 'the', 'dessert', 'our', 'friends', 'got', ',', 'but', 'we', 'tried', 'it', 'and', 'it', 'was', 'good', 'as', 'well', '!', 'I', 'do', 'remember', 'it', 'had', 'cinnamon', 'ice', 'cream', '.', 'One', 'of', 'the', 'things', 'I', 'like', 'about', 'The', 'Block', 'is', 'you', 'can', 'also', 'purchase', 'fresh', 'butchered', 'meats', 'here', '!', 'Great', 'place', '!']\n",
      "Sample rating:  5\n"
     ]
    }
   ],
   "source": [
    "train_tups, test_tups = utils.split_data(utils.generate_tuples_from_file(DATA_FILE_PATH, num_samples=10000), test_size=0.2)\n",
    "\n",
    "print('Training set size: ', len(train_tups[0]))\n",
    "print('Test set size: ', len(test_tups[0]))\n",
    "print('Sample tokenized review: ', train_tups[0][0])\n",
    "print('Sample rating: ', train_tups[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement FFNN with doc2vec embeddings as input\n",
    "\n",
    "tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_tups[0])]\n",
    "d2v_model = Doc2Vec(tagged_docs, vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "vectors = [d2v_model.dv[i] for i in range(len(tagged_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15405 (60.18 KB)\n",
      "Trainable params: 15405 (60.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import Recall, Precision, F1Score\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(input_dim,)))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# put in an output layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(learning_rate=.01)\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "f1_score = F1Score()\n",
    "\n",
    "# call compile here\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', recall, precision, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8412 - accuracy: 0.6444 - recall: 0.5028 - precision: 0.7429 - f1_score: 0.5436\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8439 - accuracy: 0.6416 - recall: 0.5118 - precision: 0.7322 - f1_score: 0.5489\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8385 - accuracy: 0.6462 - recall: 0.5106 - precision: 0.7443 - f1_score: 0.5508\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8445 - accuracy: 0.6468 - recall: 0.5140 - precision: 0.7436 - f1_score: 0.5527\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8469 - accuracy: 0.6428 - recall: 0.5092 - precision: 0.7299 - f1_score: 0.5550\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8258 - accuracy: 0.6562 - recall: 0.5176 - precision: 0.7521 - f1_score: 0.5596\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8380 - accuracy: 0.6372 - recall: 0.5096 - precision: 0.7316 - f1_score: 0.5422\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8374 - accuracy: 0.6442 - recall: 0.5100 - precision: 0.7355 - f1_score: 0.5484\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8459 - accuracy: 0.6438 - recall: 0.4996 - precision: 0.7332 - f1_score: 0.5573\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8510 - accuracy: 0.6392 - recall: 0.5086 - precision: 0.7278 - f1_score: 0.5444\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8369 - accuracy: 0.6500 - recall: 0.5192 - precision: 0.7402 - f1_score: 0.5556\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8494 - accuracy: 0.6478 - recall: 0.5118 - precision: 0.7383 - f1_score: 0.5530\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8569 - accuracy: 0.6424 - recall: 0.5100 - precision: 0.7286 - f1_score: 0.5459\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8402 - accuracy: 0.6502 - recall: 0.5132 - precision: 0.7412 - f1_score: 0.5643\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8426 - accuracy: 0.6410 - recall: 0.5086 - precision: 0.7278 - f1_score: 0.5513\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8436 - accuracy: 0.6390 - recall: 0.5144 - precision: 0.7332 - f1_score: 0.5473\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8444 - accuracy: 0.6542 - recall: 0.5120 - precision: 0.7451 - f1_score: 0.5599\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8498 - accuracy: 0.6434 - recall: 0.5068 - precision: 0.7290 - f1_score: 0.5537\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8340 - accuracy: 0.6470 - recall: 0.5170 - precision: 0.7367 - f1_score: 0.5518\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8363 - accuracy: 0.6470 - recall: 0.5112 - precision: 0.7439 - f1_score: 0.5510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a1e1d790>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Convert input data to NumPy arrays\n",
    "vectors_np = np.array(vectors)\n",
    "train_tups_np = train_tups[1]\n",
    "train_tups_np = utils.get_one_hot_encodings(train_tups_np)\n",
    "\n",
    "# Create a tf.data.Dataset from the input and output vectors\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectors_np, train_tups_np))\n",
    "\n",
    "# Repeat the dataset indefinitely\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "# Batch the data\n",
    "batch_size = 5000\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Fit the model\n",
    "steps_per_epoch = len(vectors_np) // batch_size\n",
    "model.fit(dataset, epochs=20, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Predicted rating:  2\n"
     ]
    }
   ],
   "source": [
    "review_to_predict = \"The food was awful.\"\n",
    "vectors_np = np.array([d2v_model.infer_vector(review_to_predict.split(' '))])\n",
    "prob_distribution = model.predict(vectors_np)\n",
    "\n",
    "print('Predicted rating: ', np.argmax(prob_distribution) + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "Predicted rating:  4\n"
     ]
    }
   ],
   "source": [
    "review_to_predict = \"The waiter was nice. The food was warm and delicious. I would definitely come back again.\"\n",
    "vectors_np = np.array([d2v_model.infer_vector(review_to_predict.split(' '))])\n",
    "prob_distribution = model.predict(vectors_np)\n",
    "\n",
    "print('Predicted rating: ', np.argmax(prob_distribution) + 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
